import boto3
from datetime import datetime, timedelta , timezone , date
from pyspark.context import SparkContext
from pyspark.sql import Window
from pyspark.sql.functions import *
from awsglue.context import GlueContext
from awsglue.utils import getResolvedOptions
import uuid
import logging
import sys

args = getResolvedOptions(
    sys.argv,
    [
        "JOB_NAME",
        "raw_bucket",
        "dq_bucket",
        "db_source"    
    ]
)

# Spark + Glue setup
sc = SparkContext()
glueContext = GlueContext(sc)
spark = glueContext.spark_session
spark.sparkContext.setLogLevel("INFO")

logging.basicConfig(
                      level=logging.INFO,
                      format="%(asctime)s [%(levelname)s] %(name)s: %(message)s"
)

logger = logging.getLogger(__name__)


raw_bucket = args["raw_bucket"]
dq_bucket = args["dq_bucket"]
db_name = args["db_source"]
metadata_table_name = "cdc_metadata_control"

spark.conf.set("spark.sql.catalog.glue_catalog","org.apache.iceberg.spark.SparkCatalog")
spark.conf.set("spark.sql.catalog.glue_catalog.catalog-impl", "org.apache.iceberg.aws.glue.GlueCatalog")
spark.conf.set("spark.sql.catalog.glue_catalog.warehouse",f"s3://{dq_bucket}/") 
spark.conf.set("spark.sql.catalog.glue_catalog.io-impl","org.apache.iceberg.aws.s3.S3FileIO")
spark.conf.set("spark.sql.extensions", "org.apache.iceberg.spark.extensions.IcebergsparkSessionExtensions")


logger.info(f"Starting CDC job for databse: {db_name}")



#----------------------------------
# DB_SOURCE --> S3 PREFIX_MAPPING
#----------------------------------


DB_TO_S3_PREFIX = {
 
             "ESOM_ANO" : "somano_raw.db",
             "ESOM_RBS" : "somrbs_raw.db",
             "ESOM_GGN" : "somggn_raw.db",
             "ESOM_WF3" : "somwf3_raw.db",
             "ENGAGE"   : "engage_raw.db",
             "ENGAGEFIN": "engagefin_raw.db"
}

schema_name =  DB_TO_S3_PREFIX.get(db_name) 

if not schema_name:  
       
       raise ValueError(f"unkown db_source:{db_name} -- please check mappings")
      

#--------------------------
# AWS S3 and dynamo_db client
#--------------------------
s3 = boto3.client("s3")
dynamodb = boto3.resource("dynamodb")
metadata_table = dynamodb.Table(metadata_table_name)

#--------------------------
# AUDIT FUNCTIONS
#--------------------------

def wf_insert_audit_entry(spark, audit_table, workflow_name, workflow_runid, db_name):

    event_id = str(uuid.uuid4())
    start_time = datetime.now(timezone.utc)
    spark.sql(f"""
        INSERT INTO {audit_table}
        VALUES ('{event_id}', '{workflow_name}', '{workflow_runid}', '{db_name}',
                'IN_PROGRESS', TIMESTAMP '{start_time}', NULL, NULL, NULL, NULL, NULL)
    """)
    logger.info(f"[AUDIT] Workflow started for DB: {db_name}")
    return event_id 

def wf_update_audit_entry(spark, audit_table, event_id, status, total_tables=None, total_success=None, total_failed=None, fail_reason=None):
    end_time = datetime.now(timezone.utc)
    set_clause = f"status='{status}', end_time=TIMESTAMP '{end_time}'"
    if total_tables is not None:
        set_clause += f", total_tables_processed={total_tables}"
    if total_success is not None:
        set_clause += f", total_success={total_success}"
    if total_failed is not None:
        set_clause += f", total_failed={total_failed}"
    if fail_reason is not None:
        set_clause += f", failure_reason='{fail_reason}'"
    spark.sql(f"UPDATE {audit_table} SET {set_clause} WHERE event_id='{event_id}'")
    logger.info(f"[AUDIT] Workflow {status}: {event_id}")


def insert_audit_entry(spark, audit_table, schema_name, table_name, workflow_runid):
    event_id = str(uuid.uuid4())
    start_time = datetime.now(timezone.utc)
    spark.sql(f"""
        INSERT INTO {audit_table}
        VALUES ('{event_id}', '{workflow_runid}', '{schema_name}', '{table_name}',
                'IN_PROGRESS', TIMESTAMP '{start_time}', NULL, NULL, NULL)
    """)
    logger.info(f"[AUDIT] Table started: {schema_name}.{table_name}")
    return event_id


def update_audit_entry(spark, audit_table, event_id, status, rec_count=None, fail_reason=None):
    end_time = datetime.now(timezone.utc)
    set_clause = f"status='{status}', end_time=TIMESTAMP '{end_time}'"
    if rec_count is not None:
        set_clause += f", record_count={rec_count}"
    if fail_reason is not None:
        set_clause += f", failure_reason='{fail_reason}'"
    spark.sql(f"UPDATE {audit_table} SET {set_clause} WHERE event_id='{event_id}'")
    logger.info(f"[AUDIT] Table {status}: {event_id}")



#---------------
# NORMAL FUNCTIONS
#---------------
def s3_prefix_exists(bucket, prefix):
    resp = s3.list_objects_v2(Bucket=bucket, Prefix=prefix, MaxKeys=1)
    return "Contents" in resp



# List all tables under schema folder
def list_tables_in_schema(bucket, schema):
    paginator = s3.get_paginator("list_objects_v2")
    result = paginator.paginate(Bucket=bucket, Prefix=f"{schema}/", Delimiter="/")
    tables = []
    for page in result:
        for prefix in page.get("CommonPrefixes", []):
            table_name = prefix["Prefix"].split("/")[-2]
            tables.append(table_name)
    return tables


# returns list of valid s3 paths for the given range.
def get_valid_cdc_paths( bucket, input_prefix, start_date, end_date):

    valid_paths = []
    current = start_date

    while current <= end_date:

        prefix = f"{input_prefix}{current:%Y/%m/%d}/"
        if s3_prefix_exists(raw_bucket, prefix):
            valid_paths.append(f"s3://{bucket}/{prefix}")

        current += timedelta(days=1)

    return valid_paths 



def build_merge_condition(table_name, pk_fileds):
    
    """"
      Build the ON clause for Spark SQL MERGE (Iceberg).

    """

    if not pk_fileds: 
           
           raise ValueError ("pk_fields must not be empty")
    
    conditions = []
    for pk in pk_fileds : 
        conditions.append(f"t.{pk} = s.{pk}")
    return " AND".join(conditions)



def apply_dq_rules(df, table_name, pk_fileds, iceberg_table): 
     
    cdc_df = df 
    dms_metadata_columns = ['op', 'edl_load_date'] 
    OP_COL ='op'
    target_table_cols=set(spark.table(iceberg_table).columns)

    cdc_df=cdc_df.withColumn("edl_load_date", to_timestamp(col('edl_load_date') ,"yyyy-MM-dd HH:mm:ss.SSSSSS"))
    window_spec= Window.partitionBy(*pk_fileds).orderBy(col('edl_load_date').desc())
    ranked_df  =cdc_df.withColumn('rnk',row_number().over(window_spec))
    latest_df=ranked_df.filter(col('rnk')==1)\
                         .drop('rnk','edl_load_date')   
    
    merge_condition = build_merge_condition(table_name, pk_fileds) 

    business_cols = [ c for c in latest_df.columns if c in target_table_cols and c not in set(pk_fileds) | set(dms_metadata_columns)]
    update_set = ", ".join([f"t.`{col}` = s.`{col}` for col in business_cols"])
    insert_cols = pk_fileds + business_cols


    latest_df.createOrReplaceTempView('source_table') 

    merge_sql = f""" 
                     MERGE INTO {iceberg_table} t
                     USING source_table s 
                           ON {merge_condition}
                     WHEN MATCHED AND s.op = 'D' THEN DELETE 
                     WHEN MATCHED AND s.op IN ('I','U') THEN UPDATE SET {business_cols}
                     WHEN NOT MATCHED AND s.op IN ('I','U')  THEN
                          INSERT ({", ".join([f"`{c}`" for c in insert_cols])})
                          VALUES ({", ".join([f"s.`{c}`" for c in insert_cols])})
    """

    print(f"executing merge on table {table_name}")
    spark.sql(merge_sql)
    




#------------------------
# Workflow Audit Start
#------------------------

workflow_name  = "glue_incremental_cdc"
workflow_runid = f"{db_name}_{datetime.now(timezone.utc).strftime('%Y%m%d_%H%M%S')}"
wf_event_id = wf_insert_audit_entry (
              spark,
              "glue_catalog.audit_db.etl_workflow_audit",
              workflow_name,
              workflow_runid,
              db_name
) 


#---------------------------------
# Loop through each table
#--------------------------------

success_count = 0
fail_count = 0
tables = list_tables_in_schema(raw_bucket, schema_name)
logger.info(f"Found {len(tables)} tables under schema {schema_name}: {tables}")

for table_name in tables:

    logger.info(f"Processing table: {table_name}") 

    try: 
        table_event_id=insert_audit_entry(
                        spark,
                        "glue_catalog.audit_db.etl_table_audit",
                        db_name,
                        table_name,
                        workflow_runid
        )

         # Lookup metadata record
        response = metadata_table.get_item(
               Key={
                    "db_source": db_name,
                    "table_name": table_name
               }
        )
        item = response.get("Item")

        if not item or item.get("load_type") != "cdc":
             logger.warning(f"This table is not for cdc set up . Skipping it \n")
             continue

        last_full_load_date = item.get("last_full_load_date")
        last_cdc_load_date = item.get("last_cdc_load_date")

        #generating a generalized input and output prefix
        input_prefix = f"{schema_name}/{table_name}/"
        output_prefix = f"{schema_name.replace('_raw','_dq')}/{table_name.lower()}/"
        iceberg_table = f"glue_catalog.{db_name.lower()}.{table_name.lower()}"
        logger.info(f"for table:{table_name} input_prefix:{input_prefix} and output_prefix:{output_prefix}")

        last_full_load_date = item.get("last_full_load_date")
        last_cdc_load_date = item.get("last_cdc_load_date")
        pk_fileds = item.get("pk_fields")

        today_utc = datetime.now(timezone.utc).date() 
        end_date = today_utc - timedelta(days=1)
    
    
        if last_cdc_load_date:
           start_date = date.fromisoformat(last_cdc_load_date) + timedelta(days=1)
        elif last_full_load_date:
            start_date = date.fromisoformat(last_full_load_date)
        else:
            start_date = end_date
        logger.info(f"CDC date range for table:{schema_name}.{table_name} is {start_date} to {end_date}") 

        if start_date > end_date : 
           logger.info(f"Nothing to process (start_date {start_date} > end_date {end_date}).\n")
           continue

        valid_paths=get_valid_cdc_paths(raw_bucket, input_prefix, start_date, end_date)

        if not valid_paths:
           logger.info(f"No CDC folders found for {schema_name}/{table_name}")
           continue

        logger.info(f"Found {len(valid_paths)} CDC folders to process")

        last_processed_date = None  

        #Process each CDC path
        for path in valid_paths:
          try:
              step="READ"
              logger.info(f"Reading CDC path: {path}")
              df_raw = spark.read.parquet(path) 
            
              step="MERGE"
              apply_dq_rules(df_raw, table_name, pk_fileds,iceberg_table)
              logger.info(f"Written DQ data for {table_name}")

              parts = path.rstrip("/").split("/")
              last_processed_date = f"{parts[-3]}-{parts[-2]}-{parts[-1]}"
          except Exception as e:
              logger.error(f"Error processing {path}: {e}")

          if last_processed_date:
                  metadata_table.update_item(
                      Key={"db_source": db_name, "table_name": table_name},
                      UpdateExpression="SET last_cdc_load_date = :v",
                      ExpressionAttributeValues={":v": last_processed_date}
           )
          logger.info(f"Updated last_cdc_load_date to {last_processed_date} for {table_name}")

          update_audit_entry(
              spark,
              "glue_catalog.audit_db.etl_table_audit",
              table_event_id,
              "SUCCESS",
              rec_count=df_raw.count()
          )
          success_count +=1

    except Exception as e:

        fail_count += 1
        update_audit_entry(
            spark,
            "glue_catalog.audit_db.etl_table_audit",
            table_event_id,
            "FAILED",
            fail_reason = str(e)
        )
        logger.error(f"Error Processing {db_name}.{table_name}:{e}")


wf_update_audit_entry(
    spark,
    "glue_catalog.audit_db.etl_workflow_audit",
    wf_event_id,
    "SUCCESS",
    total_tables=len(tables),
    total_success=success_count,
    total_failed=fail_count
)