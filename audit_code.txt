def wf_insert_audit_entry(spark, audit_table, workflow_name, workflow_runid, db_name):
    event_id = str(uuid.uuid4())
    start_time = datetime.now(timezone.utc)
    spark.sql(f"""
        INSERT INTO {audit_table}
        VALUES ('{event_id}', '{workflow_name}', '{workflow_runid}', '{db_name}',
                'IN_PROGRESS', TIMESTAMP '{start_time}', NULL, NULL, NULL, NULL, NULL)
    """)
    logger.info(f"[AUDIT] Workflow started for DB: {db_name}")
    return event_id


def wf_update_audit_entry(spark, audit_table, event_id, status, total_tables=None, total_success=None, total_failed=None, fail_reason=None):
    end_time = datetime.now(timezone.utc)
    set_clause = f"status='{status}', end_time=TIMESTAMP '{end_time}'"
    if total_tables is not None:
        set_clause += f", total_tables_processed={total_tables}"
    if total_success is not None:
        set_clause += f", total_success={total_success}"
    if total_failed is not None:
        set_clause += f", total_failed={total_failed}"
    if fail_reason is not None:
        set_clause += f", failure_reason='{fail_reason}'"
    spark.sql(f"UPDATE {audit_table} SET {set_clause} WHERE event_id='{event_id}'")
    logger.info(f"[AUDIT] Workflow {status}: {event_id}")


def insert_audit_entry(spark, audit_table, schema_name, table_name, workflow_runid):
    event_id = str(uuid.uuid4())
    start_time = datetime.now(timezone.utc)
    spark.sql(f"""
        INSERT INTO {audit_table}
        VALUES ('{event_id}', '{workflow_runid}', '{schema_name}', '{table_name}',
                'IN_PROGRESS', TIMESTAMP '{start_time}', NULL, NULL, NULL)
    """)
    logger.info(f"[AUDIT] Table started: {schema_name}.{table_name}")
    return event_id


def update_audit_entry(spark, audit_table, event_id, status, rec_count=None, fail_reason=None):
    end_time = datetime.now(timezone.utc)
    set_clause = f"status='{status}', end_time=TIMESTAMP '{end_time}'"
    if rec_count is not None:
        set_clause += f", record_count={rec_count}"
    if fail_reason is not None:
        set_clause += f", failure_reason='{fail_reason}'"
    spark.sql(f"UPDATE {audit_table} SET {set_clause} WHERE event_id='{event_id}'")
    logger.info(f"[AUDIT] Table {status}: {event_id}")