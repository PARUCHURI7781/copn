############################################################
# Glue Initial Full Load Job (Production Version)
# Includes:
# - Audit DB + Tables auto-creation (Iceberg)
# - Reads only root-level LOAD*.parquet files
# - Writes to Iceberg DQ zone via Glue Catalog
# - Updates only last_full_load_date in DynamoDB
############################################################

import boto3
import sys
import uuid
import logging
import re
from datetime import datetime, timezone, date
from pyspark.context import SparkContext
from awsglue.context import GlueContext
from awsglue.utils import getResolvedOptions

# --------------------------------------------------------------------------------
# Parse Arguments
# --------------------------------------------------------------------------------
args = getResolvedOptions(
    sys.argv,
    ["JOB_NAME", "raw_bucket", "dq_bucket", "db_source"]
)
raw_bucket = args["raw_bucket"]
dq_bucket = args["dq_bucket"]
db_name = args["db_source"]
metadata_table_name = "cdc_metadata_control"

# --------------------------------------------------------------------------------
# Initialize Spark + Glue
# --------------------------------------------------------------------------------
sc = SparkContext()
glueContext = GlueContext(sc)
spark = glueContext.spark_session
spark.sparkContext.setLogLevel("INFO")

logger = logging.getLogger(__name__)
logger.setLevel(logging.INFO)

# --------------------------------------------------------------------------------
# Iceberg + Glue Catalog Configuration
# --------------------------------------------------------------------------------
spark.conf.set(
    "spark.sql.extensions",
    "org.apache.iceberg.spark.extensions.IcebergSparkSessionExtensions",
)
spark.conf.set("spark.sql.catalog.glue_catalog", "org.apache.iceberg.spark.SparkCatalog")
spark.conf.set(
    "spark.sql.catalog.glue_catalog.catalog-impl",
    "org.apache.iceberg.aws.glue.GlueCatalog",
)
spark.conf.set(
    "spark.sql.catalog.glue_catalog.io-impl",
    "org.apache.iceberg.aws.s3.S3FileIO",
)
spark.conf.set("spark.sql.catalog.glue_catalog.warehouse", f"s3://{dq_bucket}/")

logger.info(f"Configured Iceberg warehouse: s3://{dq_bucket}/")

# --------------------------------------------------------------------------------
# AWS Clients
# --------------------------------------------------------------------------------
s3 = boto3.client("s3")
dynamodb = boto3.resource("dynamodb")
metadata_table = dynamodb.Table(metadata_table_name)

# --------------------------------------------------------------------------------
# DB_SOURCE → S3 Mapping
# --------------------------------------------------------------------------------
DB_TO_S3_PREFIX = {
    "ESOM_ANO": "somano_raw.db",
    "ESOM_RBS": "somrbs_raw.db",
    "ESOM_GGN": "somggn_raw.db",
    "ESOM_WF3": "somwf3_raw.db",
    "ENGAGE": "engage_raw.db",
    "ENGAGEFIN": "engagefin_raw.db",
}
schema_name = DB_TO_S3_PREFIX.get(db_name)
if not schema_name:
    raise ValueError(f"Unknown db_source: {db_name}")

# --------------------------------------------------------------------------------
# Create Audit DB + Tables if Missing
# --------------------------------------------------------------------------------
logger.info("Ensuring audit database and tables exist...")

spark.sql("CREATE DATABASE IF NOT EXISTS glue_catalog.audit_db")

spark.sql("""
CREATE TABLE IF NOT EXISTS glue_catalog.audit_db.etl_workflow_audit (
    event_id STRING,
    workflow_name STRING,
    workflow_runid STRING,
    db_name STRING,
    status STRING,
    start_time TIMESTAMP,
    end_time TIMESTAMP,
    total_tables_processed INT,
    total_success INT,
    total_failed INT,
    failure_reason STRING
) USING iceberg
""")

spark.sql("""
CREATE TABLE IF NOT EXISTS glue_catalog.audit_db.etl_table_audit (
    event_id STRING,
    workflow_runid STRING,
    schema_name STRING,
    table_name STRING,
    status STRING,
    start_time TIMESTAMP,
    end_time TIMESTAMP,
    record_count BIGINT,
    failure_reason STRING
) USING iceberg
""")

logger.info("✅ Audit database and tables ready.")

# --------------------------------------------------------------------------------
# Audit Helper Functions
# --------------------------------------------------------------------------------
def wf_insert_audit_entry(spark, audit_table, workflow_name, workflow_runid, db_name):
    event_id = str(uuid.uuid4())
    start_time = datetime.now(timezone.utc)
    spark.sql(f"""
        INSERT INTO {audit_table}
        VALUES ('{event_id}', '{workflow_name}', '{workflow_runid}', '{db_name}',
                'IN_PROGRESS', TIMESTAMP '{start_time}', NULL, NULL, NULL, NULL, NULL)
    """)
    logger.info(f"[AUDIT] Workflow started: {workflow_name}, DB: {db_name}")
    return event_id

def wf_update_audit_entry(spark, audit_table, event_id, status,
                          total_tables=None, total_success=None,
                          total_failed=None, fail_reason=None):
    end_time = datetime.now(timezone.utc)
    set_clause = f"status='{status}', end_time=TIMESTAMP '{end_time}'"
    if total_tables is not None:
        set_clause += f", total_tables_processed={total_tables}"
    if total_success is not None:
        set_clause += f", total_success={total_success}"
    if total_failed is not None:
        set_clause += f", total_failed={total_failed}"
    if fail_reason is not None:
        set_clause += f", failure_reason='{fail_reason}'"
    spark.sql(f"UPDATE {audit_table} SET {set_clause} WHERE event_id='{event_id}'")
    logger.info(f"[AUDIT] Workflow {status}: {event_id}")

def insert_audit_entry(spark, audit_table, schema_name, table_name, workflow_runid):
    event_id = str(uuid.uuid4())
    start_time = datetime.now(timezone.utc)
    spark.sql(f"""
        INSERT INTO {audit_table}
        VALUES ('{event_id}', '{workflow_runid}', '{schema_name}', '{table_name}',
                'IN_PROGRESS', TIMESTAMP '{start_time}', NULL, NULL, NULL)
    """)
    logger.info(f"[AUDIT] Table started: {schema_name}.{table_name}")
    return event_id

def update_audit_entry(spark, audit_table, event_id, status,
                       rec_count=None, fail_reason=None):
    end_time = datetime.now(timezone.utc)
    set_clause = f"status='{status}', end_time=TIMESTAMP '{end_time}'"
    if rec_count is not None:
        set_clause += f", record_count={rec_count}"
    if fail_reason is not None:
        set_clause += f", failure_reason='{fail_reason}'"
    spark.sql(f"UPDATE {audit_table} SET {set_clause} WHERE event_id='{event_id}'")
    logger.info(f"[AUDIT] Table {status}: {event_id}")

# --------------------------------------------------------------------------------
# Utilities
# --------------------------------------------------------------------------------
def list_tables_in_schema(bucket, schema):
    paginator = s3.get_paginator("list_objects_v2")
    result = paginator.paginate(Bucket=bucket, Prefix=f"{schema}/", Delimiter="/")
    tables = []
    for page in result:
        for prefix in page.get("CommonPrefixes", []):
            table_name = prefix["Prefix"].split("/")[-2]
            tables.append(table_name)
    return tables

def get_full_load_files(bucket, schema_name, table_name):
    """Return only root-level LOAD*.parquet files"""
    prefix = f"{schema_name}/{table_name}/"
    paginator = s3.get_paginator("list_objects_v2")
    page_iterator = paginator.paginate(Bucket=bucket, Prefix=prefix)
    load_files = []
    for page in page_iterator:
        for obj in page.get("Contents", []):
            key = obj["Key"]
            # Only LOAD*.parquet files at root (ignore CDC subfolders)
            if re.match(fr"{schema_name}/{table_name}/LOAD.*\.parquet$", key, re.IGNORECASE):
                load_files.append(f"s3://{bucket}/{key}")
    return load_files

# --------------------------------------------------------------------------------
# Start Workflow Audit
# --------------------------------------------------------------------------------
workflow_name = "glue_full_load"
workflow_runid = f"{db_name}_{datetime.now(timezone.utc).strftime('%Y%m%d_%H%M%S')}"
wf_event_id = wf_insert_audit_entry(
    spark, "glue_catalog.audit_db.etl_workflow_audit",
    workflow_name, workflow_runid, db_name,
)

# --------------------------------------------------------------------------------
# Full Load Loop
# --------------------------------------------------------------------------------
success_count = 0
fail_count = 0
tables = list_tables_in_schema(raw_bucket, schema_name)
logger.info(f"Discovered {len(tables)} tables under {schema_name}: {tables}")

for table_name in tables:
    table_event_id = insert_audit_entry(
        spark, "glue_catalog.audit_db.etl_table_audit",
        db_name, table_name, workflow_runid,
    )
    try:
        load_files = get_full_load_files(raw_bucket, schema_name, table_name)
        if not load_files:
            msg = f"No LOAD*.parquet files found for {schema_name}/{table_name}"
            logger.warning(msg)
            update_audit_entry(
                spark, "glue_catalog.audit_db.etl_table_audit",
                table_event_id, "FAILED", fail_reason=msg,
            )
            fail_count += 1
            continue

        logger.info(f"Found {len(load_files)} LOAD files for {table_name}")
        df_raw = spark.read.parquet(*load_files)
        record_count = df_raw.count()

        iceberg_db = db_name.lower()
        iceberg_table = f"glue_catalog.{iceberg_db}.{table_name.lower()}"

        # Ensure database exists
        spark.sql(f"CREATE DATABASE IF NOT EXISTS glue_catalog.{iceberg_db}")
        logger.info(f"Ensured database: glue_catalog.{iceberg_db}")

        # Log target path (managed automatically by Iceberg)
        target_path = f"s3://{dq_bucket}/{iceberg_db}/{table_name.lower()}/"
        logger.info(f"Writing full-load data to {target_path}")

        # Write to Iceberg (auto-creates table if missing)
        df_raw.writeTo(iceberg_table).using("iceberg").createOrReplace()
        logger.info(f"Full load complete for {db_name}.{table_name} ({record_count} records)")

        # Update audit + metadata
        update_audit_entry(
            spark, "glue_catalog.audit_db.etl_table_audit",
            table_event_id, "SUCCESS", rec_count=record_count,
        )
        metadata_table.update_item(
            Key={"db_source": db_name, "table_name": table_name},
            UpdateExpression="SET last_full_load_date = :v",
            ExpressionAttributeValues={":v": str(date.today())},
        )
        success_count += 1

    except Exception as e:
        logger.error(f"Error processing {table_name}: {e}", exc_info=True)
        update_audit_entry(
            spark, "glue_catalog.audit_db.etl_table_audit",
            table_event_id, "FAILED", fail_reason=str(e),
        )
        fail_count += 1

# --------------------------------------------------------------------------------
# End Workflow Audit
# --------------------------------------------------------------------------------
status = "SUCCESS" if fail_count == 0 else "PARTIAL_SUCCESS"
wf_update_audit_entry(
    spark, "glue_catalog.audit_db.etl_workflow_audit",
    wf_event_id, status,
    total_tables=len(tables),
    total_success=success_count,
    total_failed=fail_count,
)

logger.info(f"✅ Full load {status} for {db_name}: success={success_count}, failed={fail_count}")
